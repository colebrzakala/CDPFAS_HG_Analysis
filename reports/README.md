# Reports Directory

This directory contains all evaluation results, model comparisons, and generated figures from the cyclodextrin host-guest binding prediction project.

## Directory Structure

```
reports/
├── figures/                                    # Generated plots and visualizations
├── val_to_train_neighbors/                    # Validation-to-train neighbor analysis
├── cd_val_model_evaluation_summary.csv        # CD validation summary metrics
├── pfas_val_model_evaluation_summary.csv      # PFAS validation summary metrics
├── comprehensive_model_metrics_summary.csv    # Summary statistics across models
├── LOOCV_Results_Comparison.csv              # Leave-one-out cross-validation results (manual)
└── README.md                                   # This file
```

**Note:** `LOOCV_Results_Comparison.csv` is typically created manually from LOOCV results. The automated workflow generates `models/external_validation/loocv_final_summary.csv` which contains similar data.


## Figures Subdirectory

The `figures/` subdirectory contains generated plots and visualizations:

### Dataset Exploration
- `fig1.png` - Distribution of binding energies, host types, guest properties
- `umap_embeddings_<representation>.png` - UMAP visualizations of feature spaces

### Generating Figures

See the main [README.md](../README.md#6-visualization--figure-generation) for figure generation commands.

Quick reference for manuscript figures:
```bash
# Generate parity plot
uv run python -m cd_host_guest.plots create-parity-plot

# Generate dataset overview
uv run python -m cd_host_guest.plots plot-dataset-overview

# Generate UMAP embeddings
uv run python -m cd_host_guest.plots plot-train-val-representation-umap-grid
```

## Neighbor Analysis

The `val_to_train_neighbors/` subdirectory contains analysis of validation samples' similarity to training data.

**Purpose:** Assess how each representation relates the molecules in the training and test sets

**Files:**
- Similarity matrices between validation and training samples
- Nearest neighbor distances
- Distribution of similarities per representation

## Usage Notes

1. **CSV Files:** All CSV files can be opened in Excel, pandas, or other data analysis tools.

2. **Regenerating Reports:** Most reports can be regenerated by running evaluation commands:
   ```bash
   # Comprehensive metrics
   uv run python -m cd_host_guest.modeling.predict comprehensive-metrics --save-results

   # External validation
   uv run python -m cd_host_guest.modeling.predict evaluate-all-external-validation

   # LOOCV with hyperparameter tuning
   uv run python -m cd_host_guest.modeling.train loocv-tune-all --model-type lgbm --sweep-count 50

   # Or download best parameters from WandB and train final models
   uv run python -m cd_host_guest.modeling.train loocv-download-params
   uv run python -m cd_host_guest.modeling.train loocv-final-all --model-type lgbm
   ```

   **LOOCV workflow:**
   - `loocv-tune-all`: Runs WandB hyperparameter sweeps → Retrieves best params → Saves to JSON
   - `loocv-download-params`: Downloads best params from existing WandB sweeps (faster alternative)
   - `loocv-final-all`: Trains final models using saved parameters → Generate on external validation data (cd_val + pfas_val combined). The automated workflow generates:
   - Individual results: `models/external_validation/{representation}/loocv_results_{representation}_final.csv`
   - Individual metrics: `models/external_validation/{representation}/loocv_metrics_{representation}_final.json`
   - Summary across all representations: `models/external_validation/loocv_final_summary.csv`

   Compare LOOCV metrics with standard train/test metrics to assess robustness and generalization
   - Results saved to `models/external_validation/{representation}/` and `models/external_validation/loocv_final_summary.csv`

3. **Model Comparison:** Use `comprehensive_model_metrics.csv` to compare all models. Key metrics:
   - **MAE**: Lower is better
   - **RMSE**: Lower is better
   - **R²**: Higher is better

4. **External Validation:** Check `all_external_validation_results.csv` for generalization performance.

5. **LOOCV Results:** LOOCV provides unbiased estimates of model performance. Compare LOOCV metrics with standard train/test metrics to assess robustness.


## Data Availability

To eliminate the need to regenerate representations, models, and other results from scratch, these files have been made available. See see [README.md](../README.md) for more details.

To regenerate all results from scratch:
1. Train all models (see [README.md](../README.md#3-model-training))
2. Run comprehensive evaluation
3. Generate external validation predictions
4. Perform LOOCV experiments
5. Generate figures
